发射概率只和当前状态有关，是较简单的模型。
当遇到语料库中没有的word就返回平均概率
概率和为1，到后面乘积越来越小，然后变成0，之后就都是0了，就全部都是S
准确率0.65
------------------------------
先将语句用逗号分开，所有逗号都是S，这样可以减小长度，避免因长度过大而产生的概率消失。
同时，Pi_dic[key]和A['S'][key]应该是差不多的，key='S' or 'B'的概率和等于1，因为都不可能为'I' or 'E'，所以把逗号后面的那个字当做一个句子的开头来预测应该也差不多
准确率0.75
------------------------------
当遇到语料库中没有的word返回当前状态发射概率的最小值
准确率0.78
------------------------------
HMM2的发射概率和当前状态以及下一个状态有关，建模能力应该更强，但是……
准确率：0.76
------------------------------
HMM的viterbi_sum把乘法改成加法以解决梯度消失
准确率：0.39
原因应该在于概率没有化成对数，相对大小会发生变化
------------------------------
HMM.py加了对数处理
准确率只有0.5
------------------------------
HMM2.py加了对数处理，准确率有上升0.1左右，
dataset1有0.78，dataset2有0.82
1的train有23444行，2才只有15235行，为啥准确率还更高？
------------------------------
CRF.py，训练一个epoch
相对路径问题，是相对于顶层的路径，所以参数文件的路径最好作为一个参数传进来
准确率：0.82
------------------------------
epoch = 5
dataset1
准确率：0.85
dataset2
准确率：0.92
------------------------------
dataset1&2 epoch = 5
训练集准确率：0.9887350250938813
准确率：0.907223308403717
------------------------------
HMM两个数据集合在一起
准确率：0.81
------------------------------
epoch = 10(在原来5次的基础上迭代,62min)
dataset2:0.92，过拟合，后面5次基本就没涨
------------------------------
只留下标识两个字的模板
dataset2-epoch5-template1
train accuracy= 0.9974434018244174 valid accuracy= 0.9053367217280813 time: 35.26829883654912 min
------------------------------
unigram留下一个字的模板，bigram留下两个字的模板
dataset2-epoch5-template2
train accuracy= 0.8706735459473595 valid accuracy= 0.7992376111817027 time: 28.23314762910207 min
------------------------------
都只留下一个字的模板
dataset2-epoch5-template3
train accuracy= 0.8706735459473595 valid accuracy= 0.7992376111817027 time: 27.660934015115103 min
-------------------------------
只留下006之后的模板
dataset2-epoch5-template4
train accuracy= 0.9968801858612678 valid accuracy= 0.9040660736975857 time: 29.19031368494034 min
-------------------------------
只留下5,7,8,9
dataset2-epoch5-template5
train accuracy= 0.9969003007170946 valid accuracy= 0.8856416772554002 time: 28.41412031253179 min
U6很重要
-------------------------------
只留下5,6,8,9
dataset2-epoch5-template6
train accuracy= 0.9967145735482941 valid accuracy= 0.9047013977128335 time: 28.55057655175527 min
-------------------------------
在train_a_sentence方法里发现两处代码错误
1.更新bigram的时候用的是unigram的模板
2.if else赋值的优先级搞错了，忘记加括号了
更改之后dataset2-epoch5的结果：
train accuracy= 0.9940104664299818 valid accuracy= 0.9135959339263025 time: 59.55414261817932 min
--------------------------------
dataset1&2-epoch5
train accuracy= 0.9886820513021742 valid accuracy= 0.9167725540025413 time: 101.8819166302681 min
--------------------------------
BILSTM_CRF dataset1 epoch = 1
50min
--------------------------------
1
dataset1 对于没见过的字直接用最后一个的indice，跑一个epoch，examples上的正确率：0.7077509529860229 半个小时
--------------------------------
2 %100
dataset1 epoch = 2 0.7331639135959339
--------------------------------
3 %1000
dataset1 epoch = 5  156.04148259560267min 0.7465057179161372
主要还是数字的问题
given:
从/１９９２年/人民/出版社/成立/策划室/并/由/方/鸣/任/主任/开始/，/作为/出版/策划/的/倡言者/，/方/鸣/搞/了/一/系列/的/出版/策划/行动/，/其中/首/推/最近/几/年/陆续/出版/的/令/出版界/和/读书界/为/之/振奋/的/《/东方/书林/之/旅/》/。/这/套/由/６/个/书/系/、/２４/种/学术/文化/精品/图书/组成/的/系列/丛书/，/构成/了/近年来/中国/图书/市场/最/抢眼/的/一/道/风景线/。/
predict:
从１/９９/２年/人民/出版/社成/立策/划室/并由/方鸣任/主任/开始/，/作为/出版/策划/的/倡言者/，/方鸣/搞/了/一/系列/的/出版/策划/行动/，/其中/首推/最近/几/年/陆续/出版/的/令/出版/界和/读书/界为/之振奋/的/《/东方/书林/之旅/》/。/这/套由/６/个/书系/、/２４/种/学术/文化/精品/图书/组成/的/系列/丛书/，/构成/了/近年/来/中国/图书/市场/最/抢眼/的/一/道风/景线/。/
--------------------------------
必须在check.py中显示导入BiLSTM_CRF类，可能是pickle的一个bug
--------------------------------
4 %1000
dataset2 epoch = 1
43min
0.7738246505717916
---------------------------------
5 %5000
dataset2 epoch = 5
time: 222.814672823747 min
0.7890724269377383
---------------------------------
6 %5000 改成去概率最小的字
dataset2 epoch=5
218min
0.7960609911054638
given:
从/１９９２年/人民/出版社/成立/策划室/并/由/方/鸣/任/主任/开始/，/作为/出版/策划/的/倡言者/，/方/鸣/搞/了/一/系列/的/出版/策划/行动/，/其中/首/推/最近/几/年/陆续/出版/的/令/出版界/和/读书界/为/之/振奋/的/《/东方/书林/之/旅/》/。/这/套/由/６/个/书/系/、/２４/种/学术/文化/精品/图书/组成/的/系列/丛书/，/构成/了/近年来/中国/图书/市场/最/抢眼/的/一/道/风景线/。/
predict:
从/１９９２年/人民/出版/社成/立策/划室/并/由方/鸣任/主任/开始/，/作为/出版/策划/的/倡言者/，/方鸣搞/了/一/系列/的/出/版策/划行动/，/其中/首推/最近/几年/陆续/出版/的/令/出/版界/和/读/书界/为/之/振奋/的/《/东方/书林/之旅/》/。/这/套/由/６个/书系/、/２４种/学术/文化/精品/图书/组成/的/系列/丛书/，/构成/了/近年/来/中国/图书/市场/最/抢眼/的/一/道风/景线/。/
-------------------------------
7
epoch=4 dataset2 EMBEDDING_DIM = 10 HIDDEN_DIM = 4
0.7916137229987293
time: 215.5365016937256 min
-------------------------------
8
epoch = 3 dataset2 EMBEDDING_DIM = 5 HIDDEN_DIM = 8
time: 138.24490116834642 min 0.7852604828462516
-------------------------------
9
epoch = 1 dataset2 EMBEDDING_DIM = 5 HIDDEN_DIM = 4
torch.optim.Adam(之前全是SGD)
%1000
time: 43.900834159056345 min
0.8233799237611181
------------------------------
10 %5000
epoch = 3
time: 131.88011424541475 min
0.8119440914866582
------------------------------
11 %5000
epoch = 1 DIM=150
hidden_dim必须是偶数。否则会报错
0.85
------------------------------
12 %5000
epoch = 1 dataset1&2 EMBEDDING_DIM = 5 HIDDEN_DIM = 4
0.75
------------------------------
13 %5000
epoch=3  dataset1&2 EMBEDDING_DIM = 150 HIDDEN_DIM = 150
0.71
过拟合了
-------------------------------
14 %1000
epoch=1  dataset12 EMBEDDING_DIM = 300 HIDDEN_DIM = 300
3h 0.7325285895806861
-------------------------------
15 %1000
epoch=3 dataset12 EMBEDDING_DIM = 300 HIDDEN_DIM = 300
310min 0.72   (0.86)
-------------------------------
16
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 ADAM
60min 0.81  (0.89)
------------------------------
17
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
45min 0.87
-----------------------------
18
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=1e-4)

------------------------------
19
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-8)

------------------------------
20
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)

------------------------------
21
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-8)

------------------------------
22
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-4)

------------------------------
23
epoch=1 dataset2 EMBEDDING_DIM = 300 HIDDEN_DIM = 300 SGD
optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=1e-4)

-----------------------------
24
18->epoch=3

-----------------------------
25
18->lr=0.001

-------------------------------
26
22->lr=0.1

------------------------------
27
18->embedding=300 hidden=600